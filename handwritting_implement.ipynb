{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Neural Networks</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> A. Neural Networks for Handwritten Character Recognition </h2>\n",
    "\n",
    "<h3> Introduction </h3>\n",
    "\n",
    "The goal of this project is to implement neural network algorithm for recognizing the handwritten digits. The idea is to take a large number of training examples (handwritten digits) to automatically infer and develop rules to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1. Load Data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training data\n",
    "data_train = pd.read_csv('data/optdigits.tra',sep=',', header=None)\n",
    "train = pd.DataFrame(data_train)\n",
    "x1 = train.iloc[:,:-1]\n",
    "y1 = train.iloc[:,-1]\n",
    "x_train = x1\n",
    "y_train = pd.get_dummies(y1) #hot encode the output data of training dataset\n",
    "\n",
    "#import test data\n",
    "data_test = pd.read_csv('data/optdigits.tes',sep=',', header=None)\n",
    "test = pd.DataFrame(data_test)\n",
    "x2 = test.iloc[:,:-1]\n",
    "y2 = test.iloc[:,-1]\n",
    "x_test = x2\n",
    "y_test = pd.get_dummies(y2)  #hot encode the output data of test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2. Training </h4>\n",
    "\n",
    "<b> Model Initialization </b>\n",
    "\n",
    "First, in the __init__ function, three-layer network is constructed (input, 2-hidden layers, output) and weights and bias are assigned randomly:\n",
    "\n",
    "* Input layer - 64 neurons \n",
    "* Hidden layer 1 - 128 neurons\n",
    "* Hidden layer 2 - 128 neurons\n",
    "* Output layer - 10 neurons (output one-hot array)\n",
    "\n",
    "\n",
    "<b>Forward Propagation:</b>\n",
    "\n",
    "In forward propagation step, output is computed as follows:\n",
    "\n",
    "* z = (input * weights) + bias\n",
    "* a = activation(z)\n",
    "\n",
    "The output of the one layer will be input to the next layer and sigmoid function is used as activation function in all the layers. Here, final result is given by a3 which is output of the neural network model.\n",
    "\n",
    "<b>Backward Propagation:</b>\n",
    "\n",
    "In back propagation, weights of neural networks are modified in order to minimize the total loss function. Cost function is used to find the error i.e measure how well the network is performing compared to actual labels. This error is back propagated to all the weight matrices by computing gradients in each layer and weights are updated.\n",
    "\n",
    "<b> Prediction </b>\n",
    "\n",
    "Once the model is trained, we can perform prediction on our test dataset. Inputs are passed to feed foward network inorder to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist=[]\n",
    "class neural_network:\n",
    "    \n",
    "    def __init__(self, x, y,input_neurons,output_neurons):\n",
    "        self.inputs = x\n",
    "        self.output = y\n",
    "        self.result = np.zeros(self.output.shape)\n",
    "        neurons = 128\n",
    "        print(\"No of neurons in hidden layer\",neurons)\n",
    "        self.lr = 0.5\n",
    "        print(\"Learning Rate\",self.lr)\n",
    "        \n",
    "        #input_neurons = 64 #self.inputs.shape[1] # 64 number of input neurons\n",
    "        #output_neurons = 10 #self.outputs.shape[1]  # 10 number of output neurons\n",
    "        \n",
    "        self.input_neurons = input_neurons  #self.inputs.shape[1] # 64 number of input neurons\n",
    "        self.output_neurons = output_neurons #self.outputs.shape[1]  # 10 number of output neurons\n",
    "        \n",
    "        np.random.seed(1)\n",
    "        self.w1 = np.random.randn(input_neurons, neurons)\n",
    "        self.b1 = np.zeros((1, neurons))\n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "        self.w3 = np.random.randn(neurons, output_neurons)\n",
    "        self.b3 = np.zeros((1, output_neurons))\n",
    "        \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self,x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy(self, pred, real):\n",
    "        n_samples = real.shape[0]\n",
    "        res = pred - real\n",
    "        return res/n_samples\n",
    "\n",
    "    def cost_function(self, pred, real):\n",
    "        n_samples = real.shape[0]\n",
    "        logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "        loss = np.sum(logp)/n_samples\n",
    "        return loss\n",
    "    \n",
    "    def forward_prop(self):\n",
    "        self.a1 = self.sigmoid(np.dot(self.inputs, self.w1)+self.b1)\n",
    "        self.a2 = self.sigmoid(np.dot(self.a1, self.w2)+self.b2)\n",
    "        self.a3 = self.sigmoid(np.dot(self.a2, self.w3)+self.b3) \n",
    "        #print(self.layer1.shape, self.layer2.shape,self.result.shape)\n",
    "          \n",
    "    #function to backpropagation   \n",
    "    def backward_prop(self):\n",
    "        loss = self.cost_function(self.a3,self.output)\n",
    "        alist.append(loss)\n",
    "        a3_delta = self.cross_entropy(self.a3, self.output) # w3\n",
    "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
    "        a2_delta = z2_delta * self.sigmoid_derivative(self.a2) # w2\n",
    "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
    "        a1_delta = z1_delta * self.sigmoid_derivative(self.a1) # w1\n",
    "\n",
    "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
    "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
    "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
    "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
    "        self.w1 -= self.lr * np.dot(self.inputs.T, a1_delta)\n",
    "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
    "        return(loss)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        self.inputs = data\n",
    "        self.forward_prop()\n",
    "        #print(self.a3.argmax())\n",
    "        return self.a3.argmax()\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of neurons in hidden layer 128\n",
      "Learning Rate 0.5\n",
      "Loss is : 0.03776675938859972\n"
     ]
    }
   ],
   "source": [
    "#main program    \n",
    "model = neural_network(x_train/16.0, np.array(y_train),64,10)\n",
    "epochs = 1500 #number of epochs\n",
    "\n",
    "for x in range(epochs):\n",
    "    model.forward_prop()\n",
    "    #model.backward_prop()\n",
    "    loss = model.backward_prop()\n",
    "print('Loss is :', loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3. Evaluation </h4>\n",
    "\n",
    "The model evaluation metric accuracy was used to evaluate model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(x,y,model):\n",
    "    count = 0\n",
    "    for i in range (x.shape[0]):\n",
    "        if model.predict(x.iloc[i]) == y.iloc[i]:\n",
    "            count= count +1\n",
    "    #print('count:',count)\n",
    "    accuracy = count/x.shape[0]\n",
    "    print(\"The accuracy of model is:\", accuracy)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model is: 0.9298831385642737\n"
     ]
    }
   ],
   "source": [
    "get_accuracy(x_test,y2,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Error vs Epochs\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#x = np.arange(0, epochs, 1)\n",
    "#plt.plot(x,alist)\n",
    "#plt.xlabel('epochs')\n",
    "#plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 93% (approx)\n",
    "\n",
    "Using this model, for each 100 hand written instances, 93 were correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> B. FISHING DATA </h2>\n",
    "\n",
    "For the fishing dataset, above process were carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 9) (8, 2) (7, 9) (7, 2)\n"
     ]
    }
   ],
   "source": [
    " # read file\n",
    "file1 = pd.read_csv('data/fishing.data', names = [\"Wind\", \"Water\", \"Air\", \"Forecast\", \"class\"])\n",
    "data_1 = pd.DataFrame(file1[8:]).reset_index().drop('index', 1)\n",
    "# splitting into training and test set\n",
    "train_df = data_1.sample(frac=0.6,random_state=1321100) #random state is a seed value\n",
    "test_df = data_1.drop(train_df.index).reset_index().drop('index', 1)\n",
    "to_predict_row = pd.DataFrame([['Strong','Cold','Warm','Sunny']],columns=['Wind','Water','Air','Forecast'])\n",
    "test_df = pd.concat([test_df,to_predict_row])\n",
    "\n",
    "x1 = train_df.iloc[:,:-1]\n",
    "\n",
    "y1 = train_df.iloc[:,-1]\n",
    "x_train = pd.get_dummies(x1)\n",
    "y_train = pd.get_dummies(y1)\n",
    "#y_train = pd.get_dummies(y1) #hot encode the output data of training dataset\n",
    "\n",
    "x2 = test_df.iloc[:,:-1]\n",
    "test_df['class']= test_df['class'].map({'Yes':0, 'No':1})\n",
    "y2 = test_df.iloc[:,-1]\n",
    "x_test = pd.get_dummies(x2)\n",
    "y_test = pd.get_dummies(y2)\n",
    "print(x_train.shape,y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of neurons in hidden layer 128\n",
      "Learning Rate 0.5\n",
      "Loss is : 0.00015893897581626808\n"
     ]
    }
   ],
   "source": [
    "### main program    \n",
    "model2 = neural_network(x_train, np.array(y_train),9,2)\n",
    "epochs = 1500 #number of epochs\n",
    "\n",
    "for x in range(epochs):\n",
    "    model2.forward_prop()\n",
    "    model2.backward_prop()\n",
    "    loss = model2.backward_prop()\n",
    "print('Loss is :', loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of model is: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "get_accuracy(x_test,y2,model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Row prediction\n",
    "model2.predict(x_test.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the row: Strong,Cold,Warm,Sunny, we get predicted class as 1 i.e NO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our model correctly identified most of the output class, there are still many areas that we can tackle to further improve our model. Our model can be further improved by increasing input size and we can also experiment with optimizers like Gradient Descent and RMSProp and play around with hyperparameters of the neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
